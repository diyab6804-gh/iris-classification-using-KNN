# ğŸŒ¸ Iris Classification using K-Nearest Neighbors (KNN)

## ğŸ“Œ Project Overview
This project implements a complete **machine learning workflow for multi-class classification** using the **K-Nearest Neighbors (KNN)** algorithm on the **Iris dataset**. The goal is to understand distance-based learning, optimize model performance using hyperparameter tuning, and visualize decision boundaries for interpretability.

The project follows **industry-grade ML practices**, including feature normalization, model evaluation using confusion matrix, hyperparameter tuning, and advanced visualization techniques.

---

## ğŸ¯ Objectives
- Implement K-Nearest Neighbors (KNN) for multi-class classification  
- Normalize features to improve distance-based learning  
- Experiment with different values of K  
- Evaluate model performance using accuracy and confusion matrix  
- Visualize decision boundaries for model interpretation  

---

## ğŸ› ï¸ Tools & Technologies
- Python  
- Pandas  
- NumPy  
- Matplotlib  
- Seaborn  
- Scikit-learn  

---

## ğŸ“Š Dataset
**Dataset:** Iris Flower Dataset  

**Classes:**
- Setosa  
- Versicolor  
- Virginica  

**Features:**
- Sepal length  
- Sepal width  
- Petal length  
- Petal width  

---

## âš™ï¸ Project Workflow
1. Data Loading and Exploration  
2. Feature Normalization  
3. Train-Test Split  
4. KNN Model Training  
5. Hyperparameter Tuning (K Optimization)  
6. Model Evaluation  
7. Confusion Matrix Analysis  
8. Decision Boundary Visualization  

---

## ğŸ“ˆ Hyperparameter Tuning
Multiple values of **K (1â€“20)** are tested to identify the optimal number of neighbors, helping balance bias and variance and improving generalization.

---

## ğŸ¯ Decision Boundary Visualization
Decision boundaries are visualized using two selected features to understand how KNN separates different classes in feature space.

This provides **intuitive understanding and model interpretability**.

---

## ğŸ† Results
- High classification accuracy achieved  
- Minimal misclassifications  
- Clear decision boundary separation  
- Stable and reliable predictions  

---

## ğŸ“ Repository Structure

```
iris-classification-using-KNN/
â”‚
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ Iris dataset.csv
â”‚
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ Task6_Iris_Classification_using_KNN.ipynb
â”‚
â”œâ”€â”€ report/
â”‚   â””â”€â”€ Task6_Iris_KNN_Classification_Report.pdf
â”‚
â”œâ”€â”€ README.md

```

---

## ğŸš€ How to Run the Project

1. Clone this repository:
   ```bash
   git clone https://github.com/diyab6804-gh/iris-classification-using-KNN.git
   
2. Install dependencies
   ```
   pip install pandas numpy matplotlib seaborn scikit-learn

3. Run the Jupyter Notebook
   ```
   jupyter notebook

---

## ğŸ Conclusion

This project demonstrates the effective application of the K-Nearest Neighbors (KNN) algorithm for multi-class classification using feature normalization, hyperparameter tuning, and decision boundary visualization to achieve accurate and interpretable predictions.

---

## ğŸ‘©â€ğŸ’» Author

Patel Diya B

AI/ML Intern
